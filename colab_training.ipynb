{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adenslee/mnist_colab/blob/main/colab_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8P6Y1vUaijv"
      },
      "source": [
        "# DeepSeek-R1-Distill-Qwen-1.5B 模型微调实验\n",
        "\n",
        "本笔记本将指导您完成模型微调的整个过程。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjFkkCs4aijy"
      },
      "source": [
        "## 1. 环境准备\n",
        "首先安装必要的依赖包"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "GgO1MTE2aijz",
        "outputId": "f80eb93c-92ef-4e49-d8da-9985a6f2cb09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (4.13.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2) (1.3.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/driver.c'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-dcb482ab50e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpeft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmm_cublas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mSwitchBackLinearBnb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0;31m from .triton_based_modules import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mStandardLinear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mSwitchBackLinear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/triton_based_modules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_rowwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdequantize_rowwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from bitsandbytes.triton.int8_matmul_mixed_dequantize import (\n\u001b[1;32m      8\u001b[0m     \u001b[0mint8_matmul_mixed_dequantize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/triton/dequantize_rowwise.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# TODO: autotune this better.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     @triton.autotune(\n\u001b[0m\u001b[1;32m     19\u001b[0m         configs=[\n\u001b[1;32m     20\u001b[0m             \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_stages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_warps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/runtime/autotuner.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(fn)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/runtime/autotuner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn, arg_names, configs, key, reset_to_zero, restore_value, pre_hook, post_hook, prune_configs_by, warmup, rep, use_cuda_graph, do_bench)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     config: self.perf_model(**self.nargs, **kwargs, **config.kwargs, num_stages=config.num_stages,\n\u001b[1;32m    129\u001b[0m                                             num_warps=config.num_warps)\n\u001b[0;32m--> 130\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpruned_configs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 }\n\u001b[1;32m    132\u001b[0m                 \u001b[0mpruned_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_timing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mest_timing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/runtime/driver.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;31m# CUDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/runtime/driver.py\u001b[0m in \u001b[0;36m_initialize_obj\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"third_party\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/runtime/driver.py\u001b[0m in \u001b[0;36m_create_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cache_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDriverBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetaclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mABCMeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/driver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/driver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \"\"\"\n\u001b[1;32m   1057\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/driver.c'"
          ]
        }
      ],
      "source": [
        "# 安装依赖\n",
        "!pip install -U torch==2.1.2\n",
        "!pip install -U transformers>=4.37.2\n",
        "!pip install -U accelerate>=0.27.0\n",
        "!pip install -U peft>=0.7.0\n",
        "!pip install -U bitsandbytes>=0.41.0\n",
        "!pip install -U datasets>=2.16.0\n",
        "!pip install -U deepspeed>=0.12.0\n",
        "!pip install -U sentencepiece>=0.1.99\n",
        "!pip install -U wandb>=0.15.0\n",
        "!pip install -U trl>=0.7.10\n",
        "\n",
        "\n",
        "# 验证安装\n",
        "import torch\n",
        "import transformers\n",
        "import accelerate\n",
        "import peft\n",
        "import bitsandbytes\n",
        "import datasets\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "print(f\"PEFT version: {peft.__version__}\")\n",
        "print(f\"Bitsandbytes version: {bitsandbytes.__version__}\")\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "\n",
        "# 检查 GPU 可用性\n",
        "print(f\"\\nGPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztn1qYfaaij0"
      },
      "source": [
        "## 2. 检查 GPU 环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frfXFvNCaij0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n"
      ],
      "metadata": {
        "id": "zslrcRtavyGJ",
        "outputId": "6af3f29c-b254-4204-fd97-eb7917b3d6cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPGHIwDlaij1"
      },
      "source": [
        "## 3. 创建训练代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa8ECzFNaij1",
        "outputId": "c0fa3bde-fbfc-478b-cba2-44e9ec0c4d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_colab.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile train_colab.py\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoConfig\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType\n",
        ")\n",
        "import os\n",
        "import json\n",
        "\n",
        "def get_output_dir():\n",
        "    # 检查是否在 Colab 环境中\n",
        "    try:\n",
        "        import google.colab\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            output_dir = '/content/drive/MyDrive/model_training'\n",
        "        except:\n",
        "            output_dir = os.path.join(os.getcwd(), 'model_training')\n",
        "    except ImportError:\n",
        "        output_dir = os.path.join(os.getcwd(), 'model_training')\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    return output_dir\n",
        "\n",
        "def create_sample_dataset():\n",
        "    \"\"\"创建示例数据集\"\"\"\n",
        "    data = [\n",
        "        {\n",
        "            \"instruction\": \"解释什么是机器学习\",\n",
        "            \"input\": \"\",\n",
        "            \"output\": \"机器学习是人工智能的一个子领域，它使计算机系统能够通过经验自动改进...\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"写一个简单的Python函数\",\n",
        "            \"input\": \"计算两个数的和\",\n",
        "            \"output\": \"def add_numbers(a, b):\\n    return a + b\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"总结以下文本的主要内容\",\n",
        "            \"input\": \"人工智能是计算机科学的一个重要分支...\",\n",
        "            \"output\": \"这段文本主要讨论了人工智能的概念和应用...\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    with open('sample_data.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    \"\"\"加载模型和分词器\"\"\"\n",
        "    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "    # 配置 4-bit 量化\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    # 首先下载并加载配置\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # 设置模型类型和其他必要配置\n",
        "    config.model_type = \"qwen\"\n",
        "    config.torch_dtype = torch.float16\n",
        "    config.use_cache = True\n",
        "\n",
        "    # 加载模型\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config\n",
        "    )\n",
        "\n",
        "    # 加载分词器\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        padding_side=\"right\",\n",
        "        model_max_length=2048\n",
        "    )\n",
        "\n",
        "    # 确保 tokenizer 配置正确\n",
        "    if not tokenizer.pad_token_id:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def prepare_model_for_training(model):\n",
        "    \"\"\"准备模型进行训练\"\"\"\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=4,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        bias=\"none\",\n",
        "        inference_mode=False,\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    return model\n",
        "\n",
        "def prepare_dataset(tokenizer, data_path):\n",
        "    \"\"\"准备数据集\"\"\"\n",
        "    # 加载数据集\n",
        "    dataset = load_dataset(\"json\", data_files={\"train\": data_path})\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        \"\"\"处理数据集样本\"\"\"\n",
        "        prompts = []\n",
        "        outputs = []\n",
        "\n",
        "        # 构建提示文本\n",
        "        for instruction, input_text, output_text in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
        "            if input_text:\n",
        "                prompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput: \"\n",
        "            else:\n",
        "                prompt = f\"Instruction: {instruction}\\nOutput: \"\n",
        "\n",
        "            prompts.append(prompt)\n",
        "            outputs.append(output_text)\n",
        "\n",
        "        # 组合提示和输出\n",
        "        texts = [p + o for p, o in zip(prompts, outputs)]\n",
        "\n",
        "        # 对文本进行编码\n",
        "        encodings = tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # 设置标签\n",
        "        encodings[\"labels\"] = encodings[\"input_ids\"].clone()\n",
        "\n",
        "        return encodings\n",
        "\n",
        "    # 处理数据集\n",
        "    processed_dataset = dataset[\"train\"].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset[\"train\"].column_names,\n",
        "        batch_size=4,\n",
        "    )\n",
        "\n",
        "    return processed_dataset\n",
        "\n",
        "def main():\n",
        "    \"\"\"主函数\"\"\"\n",
        "    # 获取输出目录\n",
        "    output_dir = get_output_dir()\n",
        "\n",
        "    # 创建示例数据集\n",
        "    create_sample_dataset()\n",
        "\n",
        "    # 加载模型和分词器\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    # 准备模型进行训练\n",
        "    model = prepare_model_for_training(model)\n",
        "\n",
        "    # 准备训练数据集\n",
        "    train_dataset = prepare_dataset(tokenizer, \"sample_data.json\")\n",
        "\n",
        "    # 设置训练参数\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        save_steps=50,\n",
        "        warmup_steps=10,\n",
        "        save_total_limit=2,\n",
        "        save_safetensors=True,\n",
        "        # 添加新的参数\n",
        "        push_to_hub=False,  # 不推送到hub\n",
        "        overwrite_output_dir=True,  # 如果输出目录存在则覆盖\n",
        "    )\n",
        "\n",
        "    # 创建训练器\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    )\n",
        "\n",
        "    # 开始训练\n",
        "    trainer.train()\n",
        "\n",
        "    # 保存模型和配置\n",
        "    peft_model_path = os.path.join(output_dir, \"final_model\")\n",
        "    os.makedirs(peft_model_path, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n保存模型到: {peft_model_path}\")\n",
        "\n",
        "    # 保存 LoRA 模型和配置\n",
        "    model.save_pretrained(peft_model_path)\n",
        "\n",
        "    # 保存分词器\n",
        "    tokenizer.save_pretrained(peft_model_path)\n",
        "\n",
        "    # 保存训练参数\n",
        "    training_args.save_to_json(os.path.join(peft_model_path, \"training_args.json\"))\n",
        "\n",
        "    print(\"模型、分词器和配置已保存完成\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        model, tokenizer = main()\n",
        "    except Exception as e:\n",
        "        print(f\"训练过程中出现错误: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uin3MENHaij2"
      },
      "source": [
        "## 4. 运行训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZEU4a5qaij2",
        "outputId": "464c2e08-f651-4843-ffe2-dc21a69bfb09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/train_colab.py\", line 1, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1967, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 68, in <module>\n",
            "    from .loss.loss_utils import LOSS_MAPPING\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_utils.py\", line 21, in <module>\n",
            "    from .loss_deformable_detr import DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_deformable_detr.py\", line 4, in <module>\n",
            "    from ..image_transforms import center_to_corners_format\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\", line 21, in <module>\n",
            "    from .image_utils import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_utils.py\", line 64, in <module>\n",
            "    from torchvision import io as torchvision_io\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
            "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\", line 163, in <module>\n",
            "    @torch.library.register_fake(\"torchvision::nms\")\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module 'torch.library' has no attribute 'register_fake'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1967, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\", line 36, in <module>\n",
            "    from .. import PreTrainedModel, TFPreTrainedModel\n",
            "  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1955, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1969, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
            "module 'torch.library' has no attribute 'register_fake'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1967, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 41, in <module>\n",
            "    from .integrations import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1955, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1969, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
            "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
            "module 'torch.library' has no attribute 'register_fake'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train_colab.py\", line 3, in <module>\n",
            "    from transformers import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1955, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1969, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\n",
            "Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
            "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
            "module 'torch.library' has no attribute 'register_fake'\n"
          ]
        }
      ],
      "source": [
        "!python train_colab.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1DwiQnTaij3"
      },
      "source": [
        "## 5. 测试微调后的模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "1Acz2Jwqaij3",
        "outputId": "4ca02480-21fe-4a0e-896c-0359515c2c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始加载模型...\n",
            "正在挂载 Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive 挂载成功！\n",
            "使用模型路径: /content/drive/MyDrive/model_training/final_model\n",
            "运行过程中出现错误：模型路径不存在: /content/drive/MyDrive/model_training/final_model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "模型路径不存在: /content/drive/MyDrive/model_training/final_model",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-df6caf41beaf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mload_and_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"运行过程中出现错误：{str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-df6caf41beaf>\u001b[0m in \u001b[0;36mload_and_test_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"模型路径不存在: {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# 检查必要文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 模型路径不存在: /content/drive/MyDrive/model_training/final_model"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel, PeftConfig\n",
        "import os\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"挂载 Google Drive\"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        print(\"正在挂载 Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive 挂载成功！\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(\"未检测到 Google Colab 环境，跳过挂载...\")\n",
        "        return False\n",
        "\n",
        "def load_and_test_model():\n",
        "    print(\"开始加载模型...\")\n",
        "\n",
        "    # 挂载 Google Drive\n",
        "    is_colab = mount_google_drive()\n",
        "\n",
        "    # 设置模型路径\n",
        "    if is_colab:\n",
        "        model_path = \"/content/drive/MyDrive/model_training/final_model\"\n",
        "    else:\n",
        "        model_path = \"/content/model_training/final_model\"\n",
        "\n",
        "    print(f\"使用模型路径: {model_path}\")\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        raise ValueError(f\"模型路径不存在: {model_path}\")\n",
        "\n",
        "    # 检查必要文件\n",
        "    required_files = [\"adapter_config.json\", \"tokenizer.json\"]\n",
        "    for file in required_files:\n",
        "        file_path = os.path.join(model_path, file)\n",
        "        if not os.path.exists(file_path):\n",
        "            raise ValueError(f\"缺少必要文件: {file_path}\")\n",
        "\n",
        "    # 配置量化参数\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    # 加载 PEFT 配置\n",
        "    print(\"加载PEFT配置...\")\n",
        "    peft_config = PeftConfig.from_pretrained(model_path)\n",
        "\n",
        "    # 加载基础模型\n",
        "    print(\"加载基础模型...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        peft_config.base_model_name_or_path,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config\n",
        "    )\n",
        "\n",
        "    # 加载分词器\n",
        "    print(\"加载分词器...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_path,  # 使用保存的分词器\n",
        "        trust_remote_code=True,\n",
        "        padding_side=\"right\",\n",
        "        model_max_length=2048\n",
        "    )\n",
        "\n",
        "    if not tokenizer.pad_token_id:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    # 加载训练好的模型\n",
        "    print(\"加载LoRA权重...\")\n",
        "    model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        model_path,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "    def generate_response(prompt):\n",
        "        # 构建输入格式\n",
        "        formatted_prompt = f\"Instruction: {prompt}\\nOutput: \"\n",
        "        print(f\"\\n生成的提示：{formatted_prompt}\")\n",
        "\n",
        "        # 编码输入\n",
        "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        # 生成回答\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=512,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.1,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        # 解码输出\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # 移除提示部分，只保留回答\n",
        "        response = response.replace(formatted_prompt, \"\").strip()\n",
        "        return response\n",
        "\n",
        "    def interactive_mode():\n",
        "        print(\"\\n进入交互模式！\")\n",
        "        print(\"输入 'quit' 或 'exit' 退出\")\n",
        "        print(\"输入 'test' 运行预定义的测试问题\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # 获取用户输入\n",
        "                user_input = input(\"\\n请输入您的问题: \").strip()\n",
        "\n",
        "                # 检查是否退出\n",
        "                if user_input.lower() in ['quit', 'exit']:\n",
        "                    print(\"再见！\")\n",
        "                    break\n",
        "\n",
        "                # 检查是否运行测试\n",
        "                if user_input.lower() == 'test':\n",
        "                    run_tests(generate_response)\n",
        "                    continue\n",
        "\n",
        "                # 如果输入为空，继续下一轮\n",
        "                if not user_input:\n",
        "                    print(\"请输入有效的问题！\")\n",
        "                    continue\n",
        "\n",
        "                # 生成回答\n",
        "                print(\"\\n正在生成回答...\")\n",
        "                response = generate_response(user_input)\n",
        "                print(f\"\\n回答: {response}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"发生错误: {str(e)}\")\n",
        "                print(\"请重试或输入 'quit' 退出\")\n",
        "\n",
        "    def run_tests(generate_func):\n",
        "        \"\"\"运行预定义的测试问题\"\"\"\n",
        "        test_prompts = [\n",
        "            \"解释什么是机器学习\",\n",
        "            \"写一个简单的Python函数来计算两个数的和\",\n",
        "            \"总结一下：人工智能是计算机科学的一个重要分支...\"\n",
        "        ]\n",
        "\n",
        "        print(\"\\n开始运行测试问题...\")\n",
        "        for prompt in test_prompts:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(f\"测试问题：{prompt}\")\n",
        "            try:\n",
        "                response = generate_func(prompt)\n",
        "                print(f\"回答：{response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"生成回答时出错：{str(e)}\")\n",
        "\n",
        "    # 启动交互模式\n",
        "    interactive_mode()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        load_and_test_model()\n",
        "    except Exception as e:\n",
        "        print(f\"运行过程中出现错误：{str(e)}\")\n",
        "        raise"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepSeek-R1-Distill-Qwen-1.5B 模型微调",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}