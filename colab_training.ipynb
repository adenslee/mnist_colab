{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8P6Y1vUaijv"
      },
      "source": [
        "# DeepSeek-R1-Distill-Qwen-1.5B 模型微调实验\n",
        "\n",
        "本笔记本将指导您完成模型微调的整个过程。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjFkkCs4aijy"
      },
      "source": [
        "## 1. 环境准备\n",
        "首先安装必要的依赖包"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgO1MTE2aijz",
        "outputId": "c380f250-7327-4139-ac2a-e432419edd27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 检查 CUDA 环境\n",
        "!nvidia-smi\n",
        "print(\"\\nCUDA 库信息:\")\n",
        "!find /usr/local/cuda* -name \"libcudart.so*\"\n",
        "!find /usr/local/cuda* -name \"libcusparse.so*\"\n",
        "!find /usr/lib/x86_64-linux-gnu -name \"libcudart.so*\"\n",
        "!find /usr/lib/x86_64-linux-gnu -name \"libcusparse.so*\"\n",
        "!ldconfig -p | grep -E 'cuda|cusparse'\n",
        "\n",
        "# 安装 CUDA 工具包\n",
        "!apt-get update && apt-get install -y cuda-cudart-11-8 cuda-libraries-11-8 libcusparse-11-8\n",
        "\n",
        "# 创建符号链接\n",
        "!ln -sf /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcusparse.so.11 /usr/local/cuda/lib64/libcusparse.so.11\n",
        "\n",
        "# 设置 CUDA 环境变量\n",
        "import os\n",
        "cuda_paths = [\n",
        "    \"/usr/local/cuda-11.8/lib64\",\n",
        "    \"/usr/local/cuda-11.8/extras/CUPTI/lib64\",\n",
        "    \"/usr/local/cuda/lib64\",\n",
        "    \"/usr/local/cuda/extras/CUPTI/lib64\",\n",
        "    \"/usr/lib/x86_64-linux-gnu\",\n",
        "    \"/usr/lib/cuda/lib64\",\n",
        "    \"/usr/lib/cuda/include\",\n",
        "    \"/usr/local/cuda/targets/x86_64-linux/lib\"\n",
        "]\n",
        "\n",
        "ld_library_path = os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
        "for path in cuda_paths:\n",
        "    if os.path.exists(path) and path not in ld_library_path:\n",
        "        ld_library_path = f\"{path}:{ld_library_path}\"\n",
        "\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = ld_library_path.rstrip(\":\")\n",
        "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
        "\n",
        "print(\"\\n环境变量设置:\")\n",
        "print(\"CUDA_HOME:\", os.environ.get(\"CUDA_HOME\"))\n",
        "print(\"LD_LIBRARY_PATH:\", os.environ.get(\"LD_LIBRARY_PATH\"))\n",
        "\n",
        "# 验证 CUDA 库\n",
        "!ldconfig\n",
        "!python -c \"import torch; print('CUDA 可用:', torch.cuda.is_available()); print('CUDA 版本:', torch.version.cuda)\"\n",
        "\n",
        "# 卸载现有的包以避免冲突\n",
        "!pip uninstall -y numpy bitsandbytes transformers torch torchvision torchaudio accelerate peft setuptools jedi sentence-transformers diffusers huggingface-hub datasets fastai timm tensorflow tensorboard jax jaxlib\n",
        "\n",
        "# 按顺序安装依赖包\n",
        "!pip install -q setuptools==68.2.2 wheel==0.41.2 pip==23.3.1\n",
        "!pip install -q numpy==1.24.3\n",
        "!pip install -q torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q huggingface-hub==0.20.3\n",
        "!pip install -q transformers==4.31.0\n",
        "!pip install -q datasets==2.14.7\n",
        "!pip install -q accelerate==0.20.3 peft==0.3.0\n",
        "\n",
        "# 从源代码编译并安装 bitsandbytes\n",
        "!git clone https://github.com/TimDettmers/bitsandbytes.git\n",
        "!cd bitsandbytes && CUDA_VERSION=118 make cuda11x && python setup.py install\n",
        "\n",
        "# 安装完整的 bitsandbytes 包\n",
        "!pip install -q bitsandbytes==0.41.1\n",
        "!pip install -q triton==2.0.0\n",
        "\n",
        "# 重新导入以确保环境正确\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "if 'transformers' in sys.modules:\n",
        "    importlib.reload(sys.modules['transformers'])\n",
        "if 'numpy' in sys.modules:\n",
        "    importlib.reload(sys.modules['numpy'])\n",
        "\n",
        "# 验证 bitsandbytes 安装\n",
        "print(\"\\n验证 bitsandbytes 安装:\")\n",
        "import bitsandbytes as bnb\n",
        "print(\"已加载的 bitsandbytes 模块:\", dir(bnb))\n",
        "print(\"Bitsandbytes 版本:\", bnb.__version__)\n",
        "\n",
        "# 验证 8-bit 量化功能\n",
        "print(\"\\n验证 8-bit 量化功能:\")\n",
        "import torch\n",
        "x = torch.randn(2, 3).cuda()\n",
        "linear_8bit = bnb.nn.Linear8bitLt(3, 4, has_fp16_weights=False).cuda()\n",
        "out = linear_8bit(x)\n",
        "print(\"8-bit 线性层测试成功!\")\n",
        "\n",
        "# 验证 transformers 功能\n",
        "print(\"\\n验证 transformers 功能:\")\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
        "print(\"Transformers tokenizer 测试成功!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztn1qYfaaij0"
      },
      "source": [
        "## 2. 检查 GPU 环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frfXFvNCaij0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPGHIwDlaij1"
      },
      "source": [
        "## 3. 创建训练代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa8ECzFNaij1",
        "outputId": "793955f3-02c4-4fc4-e3e8-8d8a7ab2b441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_colab.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train_colab.py\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoConfig\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType\n",
        ")\n",
        "import os\n",
        "\n",
        "def get_output_dir():\n",
        "    # 检查是否在 Colab 环境中\n",
        "    try:\n",
        "        import google.colab\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            output_dir = '/content/drive/MyDrive/model_training'\n",
        "        except:\n",
        "            output_dir = os.path.join(os.getcwd(), 'model_training')\n",
        "    except ImportError:\n",
        "        output_dir = os.path.join(os.getcwd(), 'model_training')\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    return output_dir\n",
        "\n",
        "def create_sample_dataset():\n",
        "    data = {\n",
        "        \"instruction\": [\n",
        "            \"解释什么是机器学习\",\n",
        "            \"写一个简单的Python函数\",\n",
        "            \"总结以下文本的主要内容\"\n",
        "        ],\n",
        "        \"input\": [\n",
        "            \"\",\n",
        "            \"计算两个数的和\",\n",
        "            \"人工智能是计算机科学的一个重要分支...\"\n",
        "        ],\n",
        "        \"output\": [\n",
        "            \"机器学习是人工智能的一个子领域，它使计算机系统能够通过经验自动改进...\",\n",
        "            \"def add_numbers(a, b):\\\\n    return a + b\",\n",
        "            \"这段文本主要讨论了人工智能的概念和应用...\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open('sample_data.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump({\"train\": data}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "    # 配置 4-bit 量化\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    # 首先下载并加载配置\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # 设置模型类型和其他必要配置\n",
        "    config.model_type = \"qwen\"\n",
        "    config.torch_dtype = torch.float16\n",
        "    config.use_cache = True\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        padding_side=\"right\",\n",
        "        model_max_length=2048\n",
        "    )\n",
        "\n",
        "    # 确保 tokenizer 配置正确\n",
        "    if not tokenizer.pad_token_id:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def prepare_model_for_training(model):\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=4,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        bias=\"none\",\n",
        "        inference_mode=False,\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    return model\n",
        "\n",
        "def prepare_dataset(tokenizer, data_path):\n",
        "    dataset = load_dataset(\"json\", data_files=data_path)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        prompts = []\n",
        "        for instruction, input_text in zip(examples[\"instruction\"], examples[\"input\"]):\n",
        "            if input_text:\n",
        "                prompt = f\"Instruction: {instruction}\\\\nInput: {input_text}\\\\nOutput: \"\n",
        "            else:\n",
        "                prompt = f\"Instruction: {instruction}\\\\nOutput: \"\n",
        "            prompts.append(prompt)\n",
        "\n",
        "        texts = [p + o for p, o in zip(prompts, examples[\"output\"])]\n",
        "\n",
        "        encodings = tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return encodings\n",
        "\n",
        "    processed_dataset = dataset[\"train\"].map(\n",
        "        preprocess_function,\n",
        "        remove_columns=dataset[\"train\"].column_names,\n",
        "        batch_size=4,\n",
        "    )\n",
        "    return processed_dataset\n",
        "\n",
        "def main():\n",
        "    output_dir = get_output_dir()\n",
        "    create_sample_dataset()\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "    model = prepare_model_for_training(model)\n",
        "    train_dataset = prepare_dataset(tokenizer, \"sample_data.json\")\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        save_steps=50,\n",
        "        warmup_steps=10,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(os.path.join(output_dir, \"final_model\"))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uin3MENHaij2"
      },
      "source": [
        "## 4. 运行训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZEU4a5qaij2",
        "outputId": "683e6704-156b-4d35-962b-0cdc6a72b9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/train_colab.py\", line 2, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "2025-04-06 14:08:49.618077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743948529.839190    2695 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743948529.900426    2695 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train_colab.py\", line 140, in <module>\n",
            "    main()\n",
            "  File \"/content/train_colab.py\", line 110, in main\n",
            "    output_dir = mount_drive()\n",
            "                 ^^^^^^^^^^^^^\n",
            "  File \"/content/train_colab.py\", line 21, in mount_drive\n",
            "    drive.mount('/content/drive')\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\", line 100, in mount\n",
            "    return _mount(\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\", line 137, in _mount\n",
            "    _message.blocking_request(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\", line 173, in blocking_request\n",
            "    request_id = send_request(\n",
            "                 ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\", line 117, in send_request\n",
            "    instance = ipython.get_kernelapp()\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_ipython.py\", line 28, in get_kernelapp\n",
            "    return get_ipython().kernel.parent\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'kernel'\n"
          ]
        }
      ],
      "source": [
        "!python train_colab.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1DwiQnTaij3"
      },
      "source": [
        "## 5. 测试微调后的模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Acz2Jwqaij3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "def load_and_test_model():\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        trust_remote_code=True,\n",
        "        load_in_8bit=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    model_path = \"/content/drive/MyDrive/model_training/final_model\"\n",
        "    model = PeftModel.from_pretrained(base_model, model_path)\n",
        "\n",
        "    def generate_response(prompt):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model.generate(**inputs, max_length=256, temperature=0.7)\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    test_prompt = \"解释什么是机器学习\"\n",
        "    response = generate_response(test_prompt)\n",
        "    print(f\"问题：{test_prompt}\")\n",
        "    print(f\"回答：{response}\")\n",
        "\n",
        "load_and_test_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepSeek-R1-Distill-Qwen-1.5B 模型微调",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}