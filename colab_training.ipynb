{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek-R1-Distill-Qwen-1.5B 模型微调实验\n",
    "\n",
    "本笔记本将指导您完成模型微调的整个过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备\n",
    "首先安装必要的依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重启 Python 运行时以确保环境干净\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 CUDA 环境\n",
    "!nvidia-smi\n",
    "print(\"\\nCUDA 库信息:\")\n",
    "!find /usr/local/cuda* -name \"libcudart.so*\"\n",
    "!find /usr/local/cuda* -name \"libcusparse.so*\"\n",
    "!find /usr/lib/x86_64-linux-gnu -name \"libcudart.so*\"\n",
    "!find /usr/lib/x86_64-linux-gnu -name \"libcusparse.so*\"\n",
    "!ldconfig -p | grep -E 'cuda|cusparse'\n",
    "\n",
    "# 安装 CUDA 工具包\n",
    "!apt-get update && apt-get install -y cuda-cudart-11-8 cuda-libraries-11-8 libcusparse-11-8\n",
    "\n",
    "# 创建符号链接\n",
    "!ln -sf /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcusparse.so.11 /usr/local/cuda/lib64/libcusparse.so.11\n",
    "\n",
    "# 设置 CUDA 环境变量\n",
    "import os\n",
    "cuda_paths = [\n",
    "    \"/usr/local/cuda-11.8/lib64\",\n",
    "    \"/usr/local/cuda-11.8/extras/CUPTI/lib64\",\n",
    "    \"/usr/local/cuda/lib64\",\n",
    "    \"/usr/local/cuda/extras/CUPTI/lib64\",\n",
    "    \"/usr/lib/x86_64-linux-gnu\",\n",
    "    \"/usr/lib/cuda/lib64\",\n",
    "    \"/usr/lib/cuda/include\",\n",
    "    \"/usr/local/cuda/targets/x86_64-linux/lib\"\n",
    "]\n",
    "\n",
    "ld_library_path = os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "for path in cuda_paths:\n",
    "    if os.path.exists(path) and path not in ld_library_path:\n",
    "        ld_library_path = f\"{path}:{ld_library_path}\"\n",
    "\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = ld_library_path.rstrip(\":\")\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "\n",
    "print(\"\\n环境变量设置:\")\n",
    "print(\"CUDA_HOME:\", os.environ.get(\"CUDA_HOME\"))\n",
    "print(\"LD_LIBRARY_PATH:\", os.environ.get(\"LD_LIBRARY_PATH\"))\n",
    "\n",
    "# 验证 CUDA 库\n",
    "!ldconfig\n",
    "!python -c \"import torch; print('CUDA 可用:', torch.cuda.is_available()); print('CUDA 版本:', torch.version.cuda)\"\n",
    "\n",
    "# 卸载现有的包以避免冲突\n",
    "!pip uninstall -y numpy bitsandbytes transformers torch torchvision torchaudio accelerate peft setuptools jedi sentence-transformers diffusers huggingface-hub datasets fastai timm tensorflow tensorboard\n",
    "\n",
    "# 按顺序安装依赖包\n",
    "!pip install -q setuptools==68.2.2 wheel==0.41.2 pip==23.3.1\n",
    "!pip install -q 'numpy>=1.22,<2.1'\n",
    "!pip install -q torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q huggingface-hub==0.17.3\n",
    "!pip install -q datasets==2.14.7\n",
    "!pip install -q transformers==4.31.0 accelerate==0.20.3 peft==0.3.0\n",
    "\n",
    "# 从源代码编译并安装 bitsandbytes\n",
    "!git clone https://github.com/TimDettmers/bitsandbytes.git\n",
    "!cd bitsandbytes && CUDA_VERSION=118 make cuda11x && python setup.py install\n",
    "\n",
    "# 安装完整的 bitsandbytes 包\n",
    "!pip install -q bitsandbytes==0.41.1\n",
    "!pip install -q triton==2.0.0\n",
    "\n",
    "# 验证 bitsandbytes 安装\n",
    "print(\"\\n验证 bitsandbytes 安装:\")\n",
    "import bitsandbytes as bnb\n",
    "print(\"已加载的 bitsandbytes 模块:\", dir(bnb))\n",
    "print(\"Bitsandbytes 版本:\", bnb.__version__)\n",
    "\n",
    "# 重新导入以确保加载完整功能\n",
    "import importlib\n",
    "importlib.reload(bnb)\n",
    "\n",
    "# 验证 8-bit 量化功能\n",
    "print(\"\\n验证 8-bit 量化功能:\")\n",
    "import torch\n",
    "x = torch.randn(2, 3).cuda()\n",
    "linear_8bit = bnb.nn.Linear8bitLt(3, 4, has_fp16_weights=False).cuda()\n",
    "out = linear_8bit(x)\n",
    "print(\"8-bit 线性层测试成功!\")\n",
    "\n",
    "# 验证 transformers 功能\n",
    "print(\"\\n验证 transformers 功能:\")\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "print(\"Transformers tokenizer 测试成功!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 检查 GPU 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 创建训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_colab.py\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "import os\n",
    "\n",
    "def get_output_dir():\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        output_dir = '/content/drive/MyDrive/model_training'\n",
    "    except ImportError:\n",
    "        output_dir = os.path.join(os.getcwd(), 'model_training')\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def create_sample_dataset():\n",
    "    data = {\n",
    "        \"instruction\": [\n",
    "            \"解释什么是机器学习\",\n",
    "            \"写一个简单的Python函数\",\n",
    "            \"总结以下文本的主要内容\"\n",
    "        ],\n",
    "        \"input\": [\n",
    "            \"\",\n",
    "            \"计算两个数的和\",\n",
    "            \"人工智能是计算机科学的一个重要分支...\"\n",
    "        ],\n",
    "        \"output\": [\n",
    "            \"机器学习是人工智能的一个子领域，它使计算机系统能够通过经验自动改进...\",\n",
    "            \"def add_numbers(a, b):\\\\n    return a + b\",\n",
    "            \"这段文本主要讨论了人工智能的概念和应用...\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open('sample_data.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({\"train\": data}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    \n",
    "    # 配置 4-bit 量化\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def prepare_model_for_training(model):\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model\n",
    "\n",
    "def prepare_dataset(tokenizer, data_path):\n",
    "    dataset = load_dataset(\"json\", data_files=data_path)\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        prompts = []\n",
    "        for instruction, input_text in zip(examples[\"instruction\"], examples[\"input\"]):\n",
    "            if input_text:\n",
    "                prompt = f\"Instruction: {instruction}\\\\nInput: {input_text}\\\\nOutput: \"\n",
    "            else:\n",
    "                prompt = f\"Instruction: {instruction}\\\\nOutput: \"\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        texts = [p + o for p, o in zip(prompts, examples[\"output\"])]\n",
    "        \n",
    "        encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return encodings\n",
    "\n",
    "    processed_dataset = dataset[\"train\"].map(\n",
    "        preprocess_function,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        batch_size=4,\n",
    "    )\n",
    "    return processed_dataset\n",
    "\n",
    "def main():\n",
    "    output_dir = get_output_dir()\n",
    "    create_sample_dataset()\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "    model = prepare_model_for_training(model)\n",
    "    train_dataset = prepare_dataset(tokenizer, \"sample_data.json\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        save_steps=50,\n",
    "        warmup_steps=10,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(output_dir, \"final_model\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 运行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_colab.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 测试微调后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig\n",
    "import torch\n",
    "\n",
    "def load_and_test_model():\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    model_path = \"/content/drive/MyDrive/model_training/final_model\"\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "    def generate_response(prompt):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_length=256, temperature=0.7)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    test_prompt = \"解释什么是机器学习\"\n",
    "    response = generate_response(test_prompt)\n",
    "    print(f\"问题：{test_prompt}\")\n",
    "    print(f\"回答：{response}\")\n",
    "\n",
    "load_and_test_model()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DeepSeek-R1-Distill-Qwen-1.5B 模型微调",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
