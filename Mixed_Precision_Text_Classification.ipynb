{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 挂载 Google Drive 并设置工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 挂载 Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 创建保存结果的目录\n",
    "!mkdir -p \"/content/drive/MyDrive/mixed_precision_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 创建示例数据\n",
    "texts = [\n",
    "    \"这部电影很精彩，演员的表演非常出色\",\n",
    "    \"画面很差，故事情节也很烂\",\n",
    "    \"音乐很动听，节奏感很强\",\n",
    "    \"服务态度恶劣，等了很久才上菜\",\n",
    "    \"风景优美，空气清新，是个度假的好地方\",\n",
    "    \"价格太贵了，性价比很低\",\n",
    "    \"质量不错，用着很舒服\",\n",
    "    \"外观设计很差，做工粗糙\"\n",
    "] * 100  # 复制100次以增加数据量\n",
    "\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0] * 100  # 1表示正面评价，0表示负面评价\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"训练集大小：{len(train_df)}\")\n",
    "print(f\"测试集大小：{len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练函数（支持混合精度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device, use_amp=False, epochs=3):\n",
    "    model.train()\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    training_times = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}')\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 使用混合精度训练\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # 记录内存使用\n",
    "            memory_usage.append(torch.cuda.memory_allocated() / 1024**2)  # MB\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'memory': f'{memory_usage[-1]:.1f}MB'\n",
    "            })\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        training_times.append(epoch_time)\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}:')\n",
    "        print(f'Average Loss: {epoch_loss:.4f}')\n",
    "        print(f'Accuracy: {epoch_acc:.2f}%')\n",
    "        print(f'Time: {epoch_time:.2f}s')\n",
    "        print(f'Average Memory Usage: {sum(memory_usage)/len(memory_usage):.1f}MB\\n')\n",
    "    \n",
    "    return train_losses, train_accs, training_times, memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 比较 FP32 和 FP16 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# 初始化tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df.text.values,\n",
    "    labels=train_df.label.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 训练FP32模型\n",
    "print(\"Training with FP32...\")\n",
    "model_fp32 = TextClassifier(n_classes=2).to(device)\n",
    "optimizer_fp32 = AdamW(model_fp32.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "results_fp32 = train_model(\n",
    "    model_fp32, \n",
    "    train_loader, \n",
    "    optimizer_fp32, \n",
    "    criterion, \n",
    "    device, \n",
    "    use_amp=False, \n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# 清理内存\n",
    "del model_fp32\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# 训练FP16模型\n",
    "print(\"\\nTraining with Mixed Precision (FP16)...\")\n",
    "model_fp16 = TextClassifier(n_classes=2).to(device)\n",
    "optimizer_fp16 = AdamW(model_fp16.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "results_fp16 = train_model(\n",
    "    model_fp16, \n",
    "    train_loader, \n",
    "    optimizer_fp16, \n",
    "    criterion, \n",
    "    device, \n",
    "    use_amp=True, \n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 比较结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_comparison(results_fp32, results_fp16):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 损失对比\n",
    "    ax1.plot(results_fp32[0], label='FP32')\n",
    "    ax1.plot(results_fp16[0], label='FP16')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 准确率对比\n",
    "    ax2.plot(results_fp32[1], label='FP32')\n",
    "    ax2.plot(results_fp16[1], label='FP16')\n",
    "    ax2.set_title('Training Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 训练时间对比\n",
    "    ax3.bar(['FP32', 'FP16'], \n",
    "            [sum(results_fp32[2]), sum(results_fp16[2])],\n",
    "            alpha=0.8)\n",
    "    ax3.set_title('Total Training Time')\n",
    "    ax3.set_ylabel('Time (seconds)')\n",
    "    \n",
    "    # 内存使用对比\n",
    "    ax4.boxplot([results_fp32[3], results_fp16[3]], labels=['FP32', 'FP16'])\n",
    "    ax4.set_title('Memory Usage Distribution')\n",
    "    ax4.set_ylabel('Memory (MB)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/drive/MyDrive/mixed_precision_results/comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_comparison(results_fp32, results_fp16)\n",
    "\n",
    "# 打印详细统计信息\n",
    "print(\"\\nDetailed Statistics:\")\n",
    "print(\"FP32:\")\n",
    "print(f\"Average training time per epoch: {sum(results_fp32[2])/len(results_fp32[2]):.2f}s\")\n",
    "print(f\"Average memory usage: {sum(results_fp32[3])/len(results_fp32[3]):.1f}MB\")\n",
    "print(f\"Final accuracy: {results_fp32[1][-1]:.2f}%\")\n",
    "\n",
    "print(\"\\nFP16:\")\n",
    "print(f\"Average training time per epoch: {sum(results_fp16[2])/len(results_fp16[2]):.2f}s\")\n",
    "print(f\"Average memory usage: {sum(results_fp16[3])/len(results_fp16[3]):.1f}MB\")\n",
    "print(f\"Final accuracy: {results_fp16[1][-1]:.2f}%\")\n",
    "\n",
    "print(\"\\nPerformance Improvement:\")\n",
    "time_improvement = (sum(results_fp32[2]) - sum(results_fp16[2])) / sum(results_fp32[2]) * 100\n",
    "memory_improvement = (sum(results_fp32[3]) - sum(results_fp16[3])) / sum(results_fp32[3]) * 100\n",
    "print(f\"Time reduction: {time_improvement:.1f}%\")\n",
    "print(f\"Memory reduction: {memory_improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 推理性能比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def inference_test(model, tokenizer, text, device, use_amp=False):\n",
    "    model.eval()\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        if use_amp:\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "        else:\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    memory_used = torch.cuda.memory_allocated() / 1024**2\n",
    "    \n",
    "    probabilities = torch.softmax(outputs, dim=1)\n",
    "    prediction = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    return prediction, confidence, inference_time, memory_used\n",
    "\n",
    "# 测试文本\n",
    "test_texts = [\n",
    "    \"这家餐厅的菜品非常美味，服务也很周到\",\n",
    "    \"产品质量太差了，一点都不耐用\",\n",
    "    \"这部电影剧情紧凑，演技在线\"\n",
    "]\n",
    "\n",
    "print(\"FP32 Inference:\")\n",
    "for text in test_texts:\n",
    "    pred, conf, time_fp32, mem_fp32 = inference_test(model_fp32, tokenizer, text, device, use_amp=False)\n",
    "    print(f\"\\n文本: {text}\")\n",
    "    print(f\"预测: {'正面' if pred == 1 else '负面'}\")\n",
    "    print(f\"置信度: {conf:.2%}\")\n",
    "    print(f\"推理时间: {time_fp32*1000:.2f}ms\")\n",
    "    print(f\"内存使用: {mem_fp32:.1f}MB\")\n",
    "\n",
    "print(\"\\nFP16 Inference:\")\n",
    "for text in test_texts:\n",
    "    pred, conf, time_fp16, mem_fp16 = inference_test(model_fp16, tokenizer, text, device, use_amp=True)\n",
    "    print(f\"\\n文本: {text}\")\n",
    "    print(f\"预测: {'正面' if pred == 1 else '负面'}\")\n",
    "    print(f\"置信度: {conf:.2%}\")\n",
    "    print(f\"推理时间: {time_fp16*1000:.2f}ms\")\n",
    "    print(f\"内存使用: {mem_fp16:.1f}MB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
