{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文文本分类 - BERT 模型\n",
    "\n",
    "这个笔记本实现了一个基于 BERT 的中文文本分类模型。我们将使用 transformers 库中的中文 BERT 模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装依赖的代码单元格："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装必要的库\n",
    "!pip install transformers datasets torch scikit-learn pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库的代码单元格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挂载 Drive 的代码单元格："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 挂载 Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 创建保存结果的目录\n",
    "!mkdir -p \"/content/drive/MyDrive/text_classification_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备数据的代码单元格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建示例数据\n",
    "texts = [\n",
    "    \"这部电影很精彩，演员的表演非常出色\",\n",
    "    \"画面很差，故事情节也很烂\",\n",
    "    \"音乐很动听，节奏感很强\",\n",
    "    \"服务态度恶劣，等了很久才上菜\",\n",
    "    \"风景优美，空气清新，是个度假的好地方\",\n",
    "    \"价格太贵了，性价比很低\",\n",
    "    \"质量不错，用着很舒服\",\n",
    "    \"外观设计很差，做工粗糙\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1表示正面评价，0表示负面评价\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"训练集大小：{len(train_df)}\")\n",
    "print(f\"测试集大小：{len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# 初始化tokenizer和模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = TextClassifier(n_classes=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df.text.values,\n",
    "    labels=train_df.label.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 优化器和损失函数\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练循环\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accs.append(epoch_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}:')\n",
    "    print(f'Average Loss: {epoch_loss:.4f}')\n",
    "    print(f'Accuracy: {epoch_acc:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建测试数据集\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df.text.values,\n",
    "    labels=test_df.label.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f'测试集准确率: {test_accuracy:.2f}%')\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('混淆矩阵')\n",
    "plt.xlabel('预测标签')\n",
    "plt.ylabel('真实标签')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "save_path = '/content/drive/MyDrive/text_classification_results/bert_classifier.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "# 绘制训练过程\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('训练损失')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs)\n",
    "plt.title('训练准确率')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/text_classification_results/training_metrics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 测试自定义文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    # 对文本进行编码\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # 进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prob = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    sentiment = '正面' if predicted.item() == 1 else '负面'\n",
    "    confidence = prob[0][predicted.item()].item()\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# 测试一些新的评论\n",
    "test_texts = [\n",
    "    \"这家餐厅的菜品非常美味，服务也很周到\",\n",
    "    \"产品质量太差了，一点都不耐用\",\n",
    "    \"这部电影剧情紧凑，演技在线\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    sentiment, confidence = predict_sentiment(text)\n",
    "    print(f'\\n文本: {text}')\n",
    "    print(f'情感: {sentiment}')\n",
    "    print(f'置信度: {confidence:.2%}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Text_Classification.ipynb",
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
